{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/banner.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximum Likelihood Estimation (MLE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maximum Likelihood Estimation (MLE) is a cornerstone method in statistical inference and machine learning for estimating the parameters of a probabilistic model. It provides a principled approach to fitting models to data, based on the idea of maximizing the likelihood of observing the given data under the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The concept of maximum likelihood was first developed by R. A. Fisher in the 1920s, although similar ideas had been used earlier by Gauss and Laplace. Fisher's work formalized the method and demonstrated its wide applicability, laying the foundation for much of modern statistical theory.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At its heart, MLE asks a simple question: \"Given our observed data, what parameter values would make this data most likely to occur?\" This intuitive idea is then formalized into a rigorous mathematical framework.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mathematically, we can express the MLE estimate as:\n",
    "\n",
    "$$\\hat{\\theta}_{MLE} = \\arg\\max_{\\theta} L(\\theta|X) = \\arg\\max_{\\theta} P(X|\\theta)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where:\n",
    "- $\\hat{\\theta}_{MLE}$ is the MLE estimate of the parameter(s)\n",
    "- $L(\\theta|X)$ is the likelihood function\n",
    "- $X$ is the observed data\n",
    "- $P(X|\\theta)$ is the probability of observing $X$ given parameters $Œ∏$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üí° **Tip**: The 'arg max' notation means we're finding the value of Œ∏ that maximizes the function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLE plays a crucial role in many machine learning algorithms:\n",
    "\n",
    "1. **Model Fitting**: It provides a principled way to fit models to data.\n",
    "2. **Probabilistic Interpretations**: It allows for probabilistic interpretations of model outputs.\n",
    "3. **Theoretical Guarantees**: MLE estimators often have desirable theoretical properties like consistency and efficiency.\n",
    "4. **Foundation for Advanced Methods**: Many advanced techniques, like Expectation-Maximization (EM) algorithm, are based on MLE principles.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> üí° **Note**: Understanding MLE will give you insights into many ML algorithms' inner workings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lecture, we'll dive deeper into the mathematical foundations of MLE, explore its properties and limitations, and see how it's applied in various machine learning contexts. By the end, you'll have a solid understanding of this powerful estimation technique and be able to apply it in your own data analysis and model building tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üöÄ **Learning Goal**: By the end of this lecture, you'll be able to apply MLE to various probabilistic models and understand its role in machine learning algorithms.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understanding MLE is not just about learning a technique; it's about grasping a fundamental principle in statistical learning that will enhance your ability to work with probabilistic models and make informed decisions in data science and machine learning projects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [Fundamental Concept of Likelihood](#toc1_)    \n",
    "  - [Likelihood vs. Probability](#toc1_1_)    \n",
    "  - [Example: Coin Flipping](#toc1_2_)    \n",
    "  - [Properties of Likelihood](#toc1_3_)    \n",
    "  - [Likelihood Function](#toc1_4_)    \n",
    "- [Mathematical Formulation of MLE](#toc2_)    \n",
    "  - [Log-Likelihood Function](#toc2_1_)    \n",
    "  - [MLE Objective](#toc2_2_)    \n",
    "  - [Finding the Maximum](#toc2_3_)    \n",
    "  - [Example: Bernoulli Distribution](#toc2_4_)    \n",
    "  - [Numerical Methods](#toc2_5_)    \n",
    "- [Step-by-Step Process of MLE](#toc3_)    \n",
    "  - [Identify the Probability Distribution](#toc3_1_)    \n",
    "  - [Write the Probability Function](#toc3_2_)    \n",
    "  - [Construct the Likelihood Function](#toc3_3_)    \n",
    "  - [Take the Logarithm](#toc3_4_)    \n",
    "  - [Find the Maximum](#toc3_5_)    \n",
    "  - [Solve for the Parameters](#toc3_6_)    \n",
    "  - [Interpret the Results](#toc3_7_)    \n",
    "  - [Example: Normal Distribution](#toc3_8_)    \n",
    "  - [Practical Considerations](#toc3_9_)    \n",
    "- [MLE in Common Probability Distributions](#toc4_)    \n",
    "  - [Bernoulli Distribution](#toc4_1_)    \n",
    "  - [Binomial Distribution](#toc4_2_)    \n",
    "  - [Normal (Gaussian) Distribution](#toc4_3_)    \n",
    "  - [Poisson Distribution](#toc4_4_)    \n",
    "  - [Exponential Distribution](#toc4_5_)    \n",
    "  - [Uniform Distribution](#toc4_6_)    \n",
    "  - [Practical Example: Normal Distribution](#toc4_7_)    \n",
    "  - [Important Considerations](#toc4_8_)    \n",
    "- [Summary](#toc5_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=2\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_'></a>[Fundamental Concept of Likelihood](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understanding the concept of likelihood is crucial for grasping Maximum Likelihood Estimation. Let's dive into what likelihood means and how it differs from probability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Likelihood is a function of the parameters of a statistical model, given some observed data. It measures how well a particular set of parameter values explains the observed data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üîë **Key Takeaway**: Likelihood is about parameters, given fixed data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The likelihood is defined as the probability of observing the data given a specific set of parameter values, viewed as a function of the parameters. As such mathematically, for a parameter $\\theta$ and observed data $X$, the likelihood $L(\\theta|X)$ is proportional to the probability of observing $X$ given $\\theta$\n",
    "\n",
    "$$L(\\theta|X) \\propto P(X|\\theta)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_1_'></a>[Likelihood vs. Probability](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While likelihood and probability are related, they have distinct meanings:\n",
    "\n",
    "1. **Probability**:\n",
    "   - Describes the chance of observing data, given fixed parameters.\n",
    "   - Sums/integrates to 1 over all possible data outcomes.\n",
    "\n",
    "2. **Likelihood**:\n",
    "   - Describes the plausibility of parameter values, given fixed data.\n",
    "   - Does not sum/integrate to 1 over parameter values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üí° **Pro Tip**: Think of likelihood as \"reverse probability\" ‚Äì it's about parameters, not data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_2_'></a>[Example: Coin Flipping](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's illustrate with a coin-flipping example:\n",
    "\n",
    "- Parameter: p (probability of heads)\n",
    "- Observed data: 3 heads out of 5 flips\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Probability: P(3 heads | p = 0.5) = C(5,3) * 0.5¬≥ * 0.5¬≤ = 0.3125\n",
    "- Likelihood: L(p = 0.5 | 3 heads) ‚àù 0.5¬≥ * 0.5¬≤ = 0.03125\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So likelihood is proportional to the probability, but we're now considering p as variable and the data as fixed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_3_'></a>[Properties of Likelihood](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Non-negative**: Likelihood is always non-negative.\n",
    "2. **Relative**: Only relative values of likelihood matter, not absolute values.\n",
    "3. **Not a Probability**: Likelihood doesn't sum or integrate to 1 over parameter space.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üí° **Note**: Because only relative values matter, we often work with log-likelihood for computational convenience.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_4_'></a>[Likelihood Function](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The likelihood function is central to MLE. It's a function of the parameters, treating the observed data as fixed:\n",
    "\n",
    "$$L(\\theta) = P(X|\\theta)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For independent and identically distributed (i.i.d.) observations, the likelihood is the product of individual probabilities:\n",
    "\n",
    "$$L(\\theta) = \\prod_{i=1}^n P(x_i|\\theta)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "**IID stands for \"Independent and Identically Distributed.\"** This is a fundamental concept in probability theory and statistics, often used when describing random variables or data points. Let's break it down:\n",
    "\n",
    "- Independent:\n",
    "    - This means that the occurrence or value of one event or variable does not affect the probability of another.\n",
    "    - In other words, knowing the outcome of one event provides no information about the outcome of another event.\n",
    "\n",
    "- Identically Distributed:\n",
    "    - This means that all the random variables or data points in a set follow the same probability distribution.\n",
    "    - They all have the same underlying statistical properties (like mean and variance).\n",
    "\n",
    "When we say a set of random variables or observations is IID, we mean:\n",
    "- Each observation is independent of the others.\n",
    "- All observations come from the same probability distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üöÄ **Learning Goal**: Understand how to construct likelihood functions for different probabilistic models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In MLE, we seek to find the parameter values that maximize this likelihood function. This leads to the parameter estimates that make the observed data most probable under the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üîë **Key Takeaway**: MLE finds the parameters that make the data most likely.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understanding likelihood is fundamental to grasping how MLE works and why it's such a powerful method in statistics and machine learning. It provides a principled way to connect our models with observed data, forming the basis for parameter estimation and model fitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc2_'></a>[Mathematical Formulation of MLE](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mathematical formulation of Maximum Likelihood Estimation provides a rigorous framework for finding the best parameter estimates given observed data. Let's break down this formulation step by step.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with the likelihood function, which expresses the probability of observing the data given the parameters:\n",
    "\n",
    "$$L(\\theta|X) = P(X|\\theta)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where:\n",
    "- $L(\\theta|X)$ is the likelihood function\n",
    "- $\\theta$ represents the parameter(s) we want to estimate\n",
    "- $X$ is the observed data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For independent and identically distributed (i.i.d.) observations, the likelihood is the product of individual probabilities:\n",
    "\n",
    "$$L(\\theta|X) = \\prod_{i=1}^n P(x_i|\\theta)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_1_'></a>[Log-Likelihood Function](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, we often work with the log-likelihood function. This is because:\n",
    "1. It converts products to sums, which is computationally easier to handle.\n",
    "2. It doesn't change the location of the maximum.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The log-likelihood function is defined as:\n",
    "\n",
    "$$\\ell(\\theta|X) = \\log L(\\theta|X) = \\sum_{i=1}^n \\log P(x_i|\\theta)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üîë **Key Takeaway**: The log-likelihood is often easier to work with mathematically and computationally.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_2_'></a>[MLE Objective](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of MLE is to find the parameter values that maximize the likelihood (or log-likelihood) function:\n",
    "\n",
    "$$\\hat{\\theta}_{MLE} = \\arg\\max_{\\theta} L(\\theta|X) = \\arg\\max_{\\theta} \\ell(\\theta|X)$$\n",
    "\n",
    "Where $\\hat{\\theta}_{MLE}$ is the MLE estimate of the parameter(s).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_3_'></a>[Finding the Maximum](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the maximum, we typically follow these steps:\n",
    "\n",
    "1. Take the derivative of the log-likelihood with respect to each parameter:\n",
    "\n",
    "   $$\\frac{\\partial \\ell(\\theta|X)}{\\partial \\theta_j} = 0$$\n",
    "\n",
    "2. Solve the resulting equation(s), known as the likelihood equations:\n",
    "\n",
    "   $$\\sum_{i=1}^n \\frac{\\partial \\log P(x_i|\\theta)}{\\partial \\theta_j} = 0$$\n",
    "\n",
    "3. Check the second derivative to ensure it's a maximum, not a minimum:\n",
    "\n",
    "   $$\\frac{\\partial^2 \\ell(\\theta|X)}{\\partial \\theta_j^2} < 0$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_4_'></a>[Example: Bernoulli Distribution](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply this to a Bernoulli distribution with parameter $p$:\n",
    "\n",
    "1. Likelihood: $L(p|X) = \\prod_{i=1}^n p^{x_i}(1-p)^{1-x_i}$\n",
    "\n",
    "2. Log-likelihood: $\\ell(p|X) = \\sum_{i=1}^n [x_i \\log p + (1-x_i) \\log (1-p)]$\n",
    "\n",
    "3. Derivative: $\\frac{\\partial \\ell}{\\partial p} = \\sum_{i=1}^n [\\frac{x_i}{p} - \\frac{1-x_i}{1-p}] = 0$\n",
    "\n",
    "4. Solve: $\\hat{p}_{MLE} = \\frac{1}{n}\\sum_{i=1}^n x_i$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us the intuitive result that the MLE estimate for $p$ is the sample proportion of successes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_5_'></a>[Numerical Methods](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more complex models, closed-form solutions may not exist. In such cases, numerical optimization methods like gradient descent or Newton-Raphson are used to find the MLE estimates.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understanding this mathematical formulation is crucial for applying MLE in various contexts and for grasping more advanced statistical and machine learning concepts that build upon these foundations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc3_'></a>[Step-by-Step Process of MLE](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying Maximum Likelihood Estimation involves a systematic process. Let's break it down into clear, actionable steps that you can follow for various estimation problems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_1_'></a>[Identify the Probability Distribution](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, determine the probability distribution that best describes your data. This could be based on:\n",
    "- The nature of your data (e.g., binary, count, continuous)\n",
    "- Domain knowledge or theoretical considerations\n",
    "- Exploratory data analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: For binary outcomes, you might choose a Bernoulli distribution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_2_'></a>[Write the Probability Function](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Express the probability of observing a single data point given the parameters:\n",
    "\n",
    "$$P(x_i|\\theta)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where $x_i$ is a single observation and $\\theta$ represents the parameter(s) to be estimated.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_3_'></a>[Construct the Likelihood Function](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For independent observations, the likelihood is the product of individual probabilities:\n",
    "\n",
    "$$L(\\theta|X) = \\prod_{i=1}^n P(x_i|\\theta)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_4_'></a>[Take the Logarithm](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the likelihood to log-likelihood for computational ease:\n",
    "\n",
    "$$\\ell(\\theta|X) = \\log L(\\theta|X) = \\sum_{i=1}^n \\log P(x_i|\\theta)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üîë **Key Takeaway**: The log-likelihood converts products to sums, simplifying calculations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_5_'></a>[Find the Maximum](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the parameter values that maximize the log-likelihood:\n",
    "\n",
    "a) Take the derivative with respect to each parameter:\n",
    "\n",
    "   $$\\frac{\\partial \\ell(\\theta|X)}{\\partial \\theta_j} = 0$$\n",
    "\n",
    "b) Solve the resulting equation(s):\n",
    "\n",
    "   $$\\sum_{i=1}^n \\frac{\\partial \\log P(x_i|\\theta)}{\\partial \\theta_j} = 0$$\n",
    "\n",
    "c) Check the second derivative to ensure it's a maximum:\n",
    "\n",
    "   $$\\frac{\\partial^2 \\ell(\\theta|X)}{\\partial \\theta_j^2} < 0$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_6_'></a>[Solve for the Parameters](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the complexity of the equations:\n",
    "- For simple cases, solve analytically.\n",
    "- For complex cases, use numerical methods like gradient descent or Newton-Raphson.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_7_'></a>[Interpret the Results](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have the MLE estimates, interpret them in the context of your problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_8_'></a>[Example: Normal Distribution](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply this process to estimating the mean (Œº) and variance (œÉ¬≤) of a normal distribution:\n",
    "\n",
    "1. Probability function:\n",
    "   $$P(x_i|\\mu,\\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{(x_i-\\mu)^2}{2\\sigma^2}}$$\n",
    "\n",
    "2. Log-likelihood:\n",
    "   $$\\ell(\\mu,\\sigma^2|X) = -\\frac{n}{2}\\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2}\\sum_{i=1}^n (x_i-\\mu)^2$$\n",
    "\n",
    "3. Derivatives:\n",
    "   $$\\frac{\\partial \\ell}{\\partial \\mu} = \\frac{1}{\\sigma^2}\\sum_{i=1}^n (x_i-\\mu) = 0$$\n",
    "   $$\\frac{\\partial \\ell}{\\partial \\sigma^2} = -\\frac{n}{2\\sigma^2} + \\frac{1}{2(\\sigma^2)^2}\\sum_{i=1}^n (x_i-\\mu)^2 = 0$$\n",
    "\n",
    "4. Solve:\n",
    "   $$\\hat{\\mu}_{MLE} = \\frac{1}{n}\\sum_{i=1}^n x_i$$\n",
    "   $$\\hat{\\sigma}^2_{MLE} = \\frac{1}{n}\\sum_{i=1}^n (x_i-\\hat{\\mu})^2$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This process gives us the sample mean and variance as MLE estimates for a normal distribution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_9_'></a>[Practical Considerations](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For complex models, software packages often implement MLE algorithms.\n",
    "- In some cases, constraints on parameters may require constrained optimization techniques.\n",
    "- Always check for global vs. local maxima, especially in multi-parameter problems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By following this step-by-step process, you can apply MLE to a wide range of statistical and machine learning problems, from simple distribution fitting to complex model parameter estimation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc4_'></a>[MLE in Common Probability Distributions](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understanding how to apply MLE to common probability distributions is crucial for practical applications in statistics and machine learning. Let's explore MLE for several key distributions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc4_1_'></a>[Bernoulli Distribution](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Used for binary outcomes (success/failure).\n",
    "\n",
    "- Parameter: p (probability of success)\n",
    "- Probability mass function: P(X = x) = p^x * (1-p)^(1-x), x ‚àà {0,1}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLE solution:\n",
    "$$\\hat{p}_{MLE} = \\frac{1}{n}\\sum_{i=1}^n x_i$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üîë **Key Takeaway**: The MLE for p is simply the sample proportion of successes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc4_2_'></a>[Binomial Distribution](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extension of Bernoulli for n independent trials.\n",
    "\n",
    "- Parameters: n (number of trials), p (probability of success)\n",
    "- Probability mass function: P(X = k) = C(n,k) * p^k * (1-p)^(n-k)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLE solution:\n",
    "$$\\hat{p}_{MLE} = \\frac{\\sum_{i=1}^m k_i}{nm}$$\n",
    "where m is the number of observations and k_i is the number of successes in the i-th observation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc4_3_'></a>[Normal (Gaussian) Distribution](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fundamental for continuous data.\n",
    "\n",
    "- Parameters: Œº (mean), œÉ¬≤ (variance)\n",
    "- Probability density function: f(x) = (1 / ‚àö(2œÄœÉ¬≤)) * e^(-(x-Œº)¬≤/(2œÉ¬≤))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLE solutions:\n",
    "$$\\hat{\\mu}_{MLE} = \\frac{1}{n}\\sum_{i=1}^n x_i$$\n",
    "$$\\hat{\\sigma}^2_{MLE} = \\frac{1}{n}\\sum_{i=1}^n (x_i-\\hat{\\mu})^2$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc4_4_'></a>[Poisson Distribution](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Used for count data.\n",
    "\n",
    "- Parameter: Œª (rate)\n",
    "- Probability mass function: P(X = k) = (e^(-Œª) * Œª^k) / k!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLE solution:\n",
    "$$\\hat{\\lambda}_{MLE} = \\frac{1}{n}\\sum_{i=1}^n x_i$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc4_5_'></a>[Exponential Distribution](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often used for modeling time between events.\n",
    "\n",
    "- Parameter: Œª (rate)\n",
    "- Probability density function: f(x) = Œªe^(-Œªx), x ‚â• 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLE solution:\n",
    "$$\\hat{\\lambda}_{MLE} = \\frac{n}{\\sum_{i=1}^n x_i}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc4_6_'></a>[Uniform Distribution](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For data equally likely to occur in an interval [a,b].\n",
    "\n",
    "- Parameters: a (minimum), b (maximum)\n",
    "- Probability density function: f(x) = 1/(b-a), a ‚â§ x ‚â§ b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLE solutions:\n",
    "$$\\hat{a}_{MLE} = \\min(x_1, ..., x_n)$$\n",
    "$$\\hat{b}_{MLE} = \\max(x_1, ..., x_n)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc4_7_'></a>[Practical Example: Normal Distribution](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's work through a small example for the normal distribution:\n",
    "\n",
    "Given data: 2, 3, 5, 7, 11\n",
    "\n",
    "1. Calculate $\\hat{\\mu}_{MLE}$:\n",
    "   $$\\hat{\\mu}_{MLE} = \\frac{2 + 3 + 5 + 7 + 11}{5} = 5.6$$\n",
    "\n",
    "2. Calculate $\\hat{\\sigma}^2_{MLE}$:\n",
    "   $$\\hat{\\sigma}^2_{MLE} = \\frac{(2-5.6)^2 + (3-5.6)^2 + (5-5.6)^2 + (7-5.6)^2 + (11-5.6)^2}{5} = 11.04$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, the MLE estimates for this data are Œº ‚âà 5.6 and œÉ¬≤ ‚âà 11.04.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc4_8_'></a>[Important Considerations](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Sample Size**: MLE estimates become more reliable with larger sample sizes.\n",
    "2. **Computational Aspects**: For some distributions, numerical methods may be needed to find MLEs.\n",
    "3. **Assumptions**: Ensure your data reasonably fits the assumed distribution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understanding how to apply MLE to these common distributions provides a solid foundation for more complex modeling tasks in statistics and machine learning. It allows you to estimate parameters efficiently and make informed decisions based on your data's underlying probabilistic structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc5_'></a>[Summary](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we conclude our exploration of Maximum Likelihood Estimation, let's recap the main points and highlight the key takeaways from this lecture. Understanding these concepts will help you apply MLE effectively in your data analysis and machine learning projects.\n",
    "\n",
    "1. **Concept of Likelihood**\n",
    "   - Likelihood measures how well parameters explain observed data.\n",
    "   - It's different from probability: likelihood is about parameters given fixed data.\n",
    "\n",
    "2. **Mathematical Formulation**\n",
    "   - MLE finds parameters that maximize the likelihood function.\n",
    "   - We often work with log-likelihood for computational convenience.\n",
    "\n",
    "3. **Step-by-Step Process**\n",
    "   - Identify the probability distribution.\n",
    "   - Construct the likelihood function.\n",
    "   - Take the logarithm.\n",
    "   - Find the maximum through differentiation or numerical methods.\n",
    "\n",
    "4. **Properties of MLE**\n",
    "   - Consistency: Converges to true parameter values as sample size increases.\n",
    "   - Asymptotic normality: Estimates are approximately normally distributed for large samples.\n",
    "   - Efficiency: Achieves the Cram√©r-Rao lower bound asymptotically.\n",
    "\n",
    "5. **Applications in Machine Learning**\n",
    "   - Fundamental to many algorithms, including linear and logistic regression.\n",
    "   - Forms the basis for more advanced techniques like EM algorithm.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understanding MLE lays a strong foundation for more advanced topics in statistical learning and machine learning. It will help you in:\n",
    "\n",
    "- Grasping more complex estimation techniques.\n",
    "- Understanding the principles behind many machine learning algorithms.\n",
    "- Developing your own models and estimation procedures.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üöÄ **Final Thought**: MLE is not just a technique, but a fundamental principle in statistical inference. Mastering it will significantly enhance your ability to work with data and build effective models in various domains of data science and machine learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By internalizing these concepts and practices, you're well-equipped to apply MLE in your work and to delve deeper into the world of statistical estimation and machine learning algorithms."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
